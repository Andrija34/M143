#  M143 Projekt: Backup- und Restore-System mit AWS

##  Projekttitel
**Hybrid Cloud Backup & Restore auf AWS**

##  Projektbeschreibung
In diesem Projekt wird ein professionelles Backup- und Restore-System auf Basis von **zwei virtuellen Maschinen (VMs)** innerhalb der **AWS Cloud** implementiert.  
Das Ziel ist es, ein **zuverlÃ¤ssiges, automatisiertes und verschlÃ¼sseltes Backup-System** zu erstellen, das sowohl **lokale** als auch **Cloud-basierte Sicherungen** (Hybrid-LÃ¶sung) abbildet.  
Durch die Nutzung von **Veeam Agent for Windows** und **Duplicati** wird das System so konfiguriert, dass es:
- vollstÃ¤ndige System-Images erstellt,  
- verschlÃ¼sselte Datei-Backups in AWS S3 hochlÃ¤dt,  
- und eine Wiederherstellung auf einer zweiten VM ermÃ¶glicht.  

Diese Umgebung dient als Nachweis fÃ¼r alle Kompetenzen (A1â€“F1) des Moduls M143 *â€œDaten sichern und wiederherstellenâ€*.

---

# ğŸ§  A1 â€“ Daten klassifizieren und sichern (Advanced)

## ğŸ¯ Ziel
In diesem Schritt werden alle relevanten Daten auf der Backup-VM (**VM1**) systematisch **identifiziert, klassifiziert und strukturiert**, um gezielt zu entscheiden, **welche Daten gesichert**, **wie oft** sie gesichert werden und **welche ausgeschlossen** werden sollen.  
Dies bildet die Grundlage fÃ¼r eine effiziente und nachvollziehbare Backup-Strategie im gesamten Projekt.

---

## âš™ï¸ Umsetzung

### 1ï¸âƒ£ Aufbau der Datenstruktur
Auf der Haupt-VM (`VM1`) wurde ein zentraler Datenordner erstellt, der alle projekt- und systemspezifischen Dateien enthÃ¤lt.  
Die Struktur wurde logisch in **Dokumente**, **Logs**, **Backups**, **Konfigurationen** und **temporÃ¤re Dateien** gegliedert.

#### ğŸ’» PowerShell-Befehl zur Erstellung
```powershell
New-Item -ItemType Directory -Force C:\Data\Dokumente | Out-Null
New-Item -ItemType Directory -Force C:\Data\Logs | Out-Null
New-Item -ItemType Directory -Force C:\Data\Backups | Out-Null
New-Item -ItemType Directory -Force C:\Data\Konfig | Out-Null
New-Item -ItemType Directory -Force C:\Data\Temp | Out-Null

Set-Content C:\Data\Dokumente\Bericht.docx "Backup-Projektbericht"
Set-Content C:\Data\Logs\System.log "Logeintrag $(Get-Date)"
Set-Content C:\Data\Backups\DatabaseDump.bak "SQL Backup Platzhalter"
Set-Content C:\Data\Konfig\appsettings.json "{ `"env`": `"prod`" }"
Set-Content C:\Data\Temp\cache.tmp "temp file"
```
**Ergebnis:**  
Die Daten sind nun thematisch getrennt und kÃ¶nnen individuell gesichert oder ausgeschlossen werden.

![OrdnerStruktur](./Screenshots/Data.png)

---

### 2ï¸âƒ£ Klassifizierung der Daten
Die Daten wurden nach **KritikalitÃ¤t und Wiederherstellungsbedarf** eingeteilt.  
Ziel ist es, Ressourcen (Speicher, Zeit, Bandbreite) effizient zu nutzen, ohne wichtige Daten zu gefÃ¤hrden.

| Kategorie | Beispiele | Schutzbedarf | Backup-Art | Aufbewahrung |
|------------|------------|---------------|-------------|---------------|
| **Kritisch** | `C:\Data\Konfig\appsettings.json`, Systemeinstellungen, Lizenzdateien | Hoch | **Image-Backup (Veeam)** | 14 Restore Points |
| **Wichtig** | `C:\Data\Dokumente\*`, `C:\Data\Backups\*.bak` | Mittel | **Datei-Backup (Duplicati, AES-256)** | 7d / 4w / 12m |
| **Unkritisch** | `C:\Data\Temp\*`, `*.tmp`, `*.cache` | Niedrig | Wird **nicht gesichert** | â€“ |

---

### ğŸ§© BegrÃ¼ndung der Klassifizierung
- **Kritische Daten** mÃ¼ssen sofort und vollstÃ¤ndig wiederherstellbar sein (SystemintegritÃ¤t).  
- **Wichtige Daten** sind inhaltlich relevant, Ã¤ndern sich hÃ¤ufig und werden daher versioniert in die Cloud gesichert.  
- **Unkritische Dateien** verursachen nur unnÃ¶tigen Speicherverbrauch und werden explizit ausgeschlossen.

---

### 3ï¸âƒ£ Definition der Ausschlussregeln
Um Cloud-Speicherplatz zu sparen und die Wiederherstellung zu beschleunigen, wurden folgende **Ausschlussregeln** definiert:

```
C:\Data\Temp
*.tmp
*.cache
**\node_modules
C:\Users*\AppData\Local\Temp\
```

Diese Regeln werden spÃ¤ter direkt in **Duplicati** und optional in **Veeam File-Level Restore** integriert.  
So wird vermieden, dass temporÃ¤re oder automatisch generierte Daten unnÃ¶tig gesichert werden.


# ğŸ§  A2 â€“ Risikoanalyse und Backup-Strategie (Advanced)

## ğŸ¯ Ziel
In diesem Schritt wird eine vollstÃ¤ndige **Risikoanalyse** durchgefÃ¼hrt und darauf aufbauend eine **Backup-Strategie** entwickelt.  
Ziel ist, die **DatenverfÃ¼gbarkeit, IntegritÃ¤t und Vertraulichkeit** sicherzustellen â€“ auch bei SystemausfÃ¤llen oder Datenverlust.  

Im Unterschied zu A1 liegt der Fokus hier auf der **strategischen Planung und technischen Umsetzung** der 3-2-1-Backup-Regel mit einer lokalen Cloud-LÃ¶sung (**MinIO** als S3-Ersatz).

---

## âš™ï¸ Umsetzung

### 1ï¸âƒ£ Risikoanalyse
Zur Bewertung potenzieller Bedrohungen wurde eine Risikoanalyse erstellt.  
Sie dient als Grundlage fÃ¼r die Auswahl der Backup-Strategie.

| Risiko | Ursache | Wahrscheinlichkeit | Auswirkung | GegenmaÃŸnahme |
|---------|----------|--------------------|-------------|----------------|
| Hardwareausfall | Defekte HDD/SSD | Mittel | Datenverlust | Lokales Backup (Veeam) |
| Virus / Ransomware | Schadsoftware, E-Mail | Hoch | Totalverlust | Cloud-Backup mit Duplicati |
| Benutzerfehler | Unachtsames LÃ¶schen | Mittel | Teilverlust | Versionierung aktivieren |
| Feuer / Diebstahl | Physischer Schaden | Niedrig | Komplettverlust | Offsite-Backup (Cloud) |
| Softwarefehler | Fehlkonfiguration | Mittel | Systemausfall | Image-Backup & Restore-Test |

ğŸ“„ **Datei gespeichert als:**  
`C:\Data\Risikoanalyse_A2.txt`

ğŸ’¾ *Screenshot:*  
![Risikoanalyse](./Screenshots/Risikoanalyse.png)

---

### 2ï¸âƒ£ RPO & RTO Definition

| Kennzahl | Bedeutung | Wert | BegrÃ¼ndung |
|-----------|------------|-------|-------------|
| **RPO** (Recovery Point Objective) | Maximal tolerierter Datenverlust | 4 Stunden | TÃ¤gliche Sicherung + Versionierung |
| **RTO** (Recovery Time Objective) | Maximal zulÃ¤ssige Wiederherstellungszeit | 1 Stunde | Daten lokal & Cloud verfÃ¼gbar |

ğŸ“„ **Datei gespeichert als:**  
`C:\Data\RPO_RTO_A2.txt`

ğŸ’¾ *Screenshot:*  
![RPO_RTO](./Screenshots/RPO_RTO.png)

---

### 3ï¸âƒ£ Umsetzung der 3-2-1-Backup-Strategie

Die 3-2-1-Regel besagt:  
- **3 Kopien** der Daten  
- **2 unterschiedliche Speichermedien**  
- **1 Kopie auÃŸerhalb des Systems (Offsite)**

| Kopie | Speicherort | Medium | Tool |
|--------|--------------|---------|------|
| 1ï¸âƒ£ | `C:\Data` | Hauptspeicher | â€“ |
| 2ï¸âƒ£ | `D:\Backup` *(optional)* | Lokale HDD | Veeam |
| 3ï¸âƒ£ | `MinIO â€“ Bucket: backup-m143` | Virtuelle Cloud | Duplicati (AES-256) |

---

### 4ï¸âƒ£ Einrichtung der lokalen Cloud (MinIO)

Da auf AWS keine IAM-Rollen erstellt werden konnten, wurde **MinIO** als lokaler, S3-kompatibler Server eingesetzt.  
Er lÃ¤uft auf Port **9000 (API)** und **9001 (Konsole)**.

#### ğŸ’» PowerShell-Befehl:
```powershell
cd C:\minio
.\minio.exe server C:\minio\data --console-address ":9001"
```

Nach dem Start:

```
AccessKey: minioadmin
SecretKey: minioadmin
```

ğŸ§© **Web-Konsole:**  
[http://localhost:9001](http://localhost:9001)

ğŸ“¦ **Bucket erstellt:**  
`backup-m143`

ğŸ’¾ **Screenshot:**  
![Minio_Bucket](./Screenshots/Minio_Bucket.png)

---

## 5ï¸âƒ£ Cloud-Backup mit Duplicati

### ğŸ”§ Verbindung
Duplicati wurde auf der VM eingerichtet und Ã¼ber das Webinterface unter  
[http://localhost:8200](http://localhost:8200) konfiguriert.

### ğŸ“‹ Backup-Job Einstellungen

| Einstellung | Wert |
|--------------|------|
| **Name** | MinIO Cloud Backup |
| **Storage Type** | S3 compatible |
| **Server URL** | `http://localhost:9000` |
| **Bucket Name** | `backup-m143` |
| **Region** | `eu-local-1` |
| **Access Key** | `minioadmin` |
| **Secret Key** | `minioadmin` |
| **VerschlÃ¼sselung** | AES-256 |
| **Zeitplan** | TÃ¤glich 22:00 Uhr |
| **Aufbewahrung** | 7D:1D, 4W:1W, 12M:1M |

ğŸ’¾ **Screenshots:**
![Duplicati Connection](./Screenshots/Duplicati_connection.png)
![Cuplicati Filter](./Screenshots/Duplicati_filter.png)
![Duplicati Schedule](./Screenshots/Duplicati_Schedule.png)

## 6ï¸âƒ£ Ausschlussregeln

Die aus A1 bekannten Filter wurden Ã¼bernommen, um temporÃ¤re oder redundante Daten auszuschlieÃŸen.

```
C:\Data\Temp
*.tmp
*.cache
**\node_modules
C:\Users*\AppData\Local\Temp\
```

Diese Regeln reduzieren Speicherverbrauch und Upload-Zeit.

---

## 7ï¸âƒ£ Backup-Test und Wiederherstellung

### ğŸ§© Testdatei erstellt:
```powershell
echo "MinIO-Testdatei" > C:\Data\Dokumente\Test_A2.txt
```

Duplicati-Backup manuell gestartet âœ…

Datei gelÃ¶scht

Duplicati â†’ Restore â†’ Test_A2.txt wiederhergestellt

âœ… Die Datei konnte erfolgreich wiederhergestellt werden.

ğŸ’¾ Screenshots:

![Backup Success](./Screenshots/Backup_Success.png)

ğŸ§© Fachliche BegrÃ¼ndung (Advanced-Niveau)
Durch den Einsatz von MinIO wurde ein Cloud-System aufgebaut, das S3-kompatibel ist.

Die Daten sind mit AES-256 verschlÃ¼sselt, womit Vertraulichkeit gewÃ¤hrleistet wird.

Die 3-2-1-Regel sorgt fÃ¼r Redundanz und hohe VerfÃ¼gbarkeit.

RPO/RTO-Ziele wurden definiert und technisch umgesetzt.

Ausschlussregeln verbessern die Effizienz und senken Speicherbedarf.

Diese Kombination erfÃ¼llt die Anforderungen des Kompetenzrasters M143 (Advanced):
Planung, Umsetzung, Test und Dokumentation einer vollstÃ¤ndigen Backup-LÃ¶sung mit nachvollziehbarer Sicherheitsstrategie.

ğŸ§¾ Zusammenfassung
Teil	            Ergebnis
Risikoanalyse	    Dokumentiert & bewertet
Backup-Strategie	3-2-1-Regel umgesetzt
Cloud-Backup	    MinIO + Duplicati mit AES-256
Wiederherstellung	Erfolgreich getestet
Dokumentation	    VollstÃ¤ndig mit Screenshots



# ğŸ§  B1 â€“ Backup implementieren und Ã¼berwachen (Advanced)

## ğŸ¯ Ziel
In diesem Schritt wird das zuvor geplante Backup-System aus A2 **implementiert**, **automatisiert** und **Ã¼berwacht**.  
Dadurch wird sichergestellt, dass Backups regelmÃ¤ÃŸig ausgefÃ¼hrt, dokumentiert und Fehler rechtzeitig erkannt werden.  

Das Ziel ist, eine zuverlÃ¤ssige und nachvollziehbare Backup-Ãœberwachung zu gewÃ¤hrleisten.

---

## âš™ï¸ Umsetzung

### 1ï¸âƒ£ Automatische Backups prÃ¼fen
Es wird geprÃ¼ft, ob Duplicati die Sicherungen automatisch ausfÃ¼hrt.

1. Duplicati Ã¶ffnen â†’ [http://localhost:8200](http://localhost:8200)
2. Backup-Job **â€MinIO Cloud Backupâ€œ** Ã¶ffnen  
3. **Edit â†’ Schedule (Zeitplan)**  
   - âœ… â€Automatically run backupsâ€œ aktiv  
   - ğŸ•’ Zeit: 22:00 Uhr  
4. Sicherstellen, dass kein Pause-Zeitfenster aktiviert ist  

ğŸ’¾ **Screenshot:**  
![Schedule_Check](./Screenshots/Duplicati_Schedule_Check.png)

---

### 2ï¸âƒ£ Backup-Logs aktivieren
Damit die Sicherungen nachvollziehbar sind, werden alle Aktionen in einer Log-Datei gespeichert.

**Einstellungen in Duplicati â†’ Settings â†’ Advanced options:**

```
--log-file=C:\Data\Logs\Duplicati.log
--log-level=Information
```

Nach dem nÃ¤chsten Backup findet man im Log:

```
[INFO] Backup completed successfully at 2025-11-04 02:24:00
```

ğŸ’¾ **Screenshot:**  
![Log](./Screenshots/Duplicati_log.png)

---

### 3ï¸âƒ£ FehlerÃ¼berwachung einrichten
Ziel ist, Fehler und Warnungen sichtbar zu machen â€“ entweder per E-Mail oder lokal.

#### Variante A â€“ E-Mail-Report
Falls Internet verfÃ¼gbar ist, kann eine Benachrichtigung bei Fehlern aktiviert werden:

```
--send-mail-url=smtp://smtp.gmail.com:587
--send-mail-any-operation=true
--send-mail-to=deine-mail@gmail.com

--send-mail-from=duplicati@vm1.local

--send-mail-username=deinmailname
--send-mail-password=deinpasswort
--send-mail-subject=Duplicati Backup Report
```

ğŸ’¾ **Screenshot:**  
![Report](./Screenshots/Report.png)

---

5ï¸âƒ£ Backup-Monitoring-Dashboard prÃ¼fen

Duplicati bietet eine grafische Ãœbersicht aller SicherungslÃ¤ufe.

Im Dashboard sind sichtbar:
```
Letztes Backup (Datum/Uhrzeit)

Dauer

Datenmenge

Status (âœ”ï¸ erfolgreich / âŒ Fehler)
```

Ãœber â€Show logâ€œ erhÃ¤lt man detaillierte Informationen zu jedem Lauf.

ğŸ’¾ Screenshot:
![Dashboard](./Screenshots/Dashboard.png)

ğŸ§© Fachliche BegrÃ¼ndung (Advanced-Niveau)

Automatisierung: Die Sicherung erfolgt zeitgesteuert, ohne manuelles Eingreifen.

Nachvollziehbarkeit: Logs und Ereignisanzeige ermÃ¶glichen Fehleranalyse.

VerfÃ¼gbarkeit: Restore-Tests belegen Wiederherstellbarkeit.

Monitoring: Ãœberwachung im Dashboard stellt Funktion sicher.

Sicherheit: VerschlÃ¼sselung (AES-256) und Ausschlussregeln erhÃ¶hen Effizienz.

Diese Punkte erfÃ¼llen die Kriterien des Kompetenzrasters M143 (Advanced) â€“
eigenstÃ¤ndige Planung, Umsetzung, Ãœberwachung und Kontrolle einer Backup-LÃ¶sung.

ğŸ§¾ Zusammenfassung
Teil	            Ergebnis
Zeitplanung	        Automatisches Backup aktiv
Protokollierung	    Log-Datei in C:\Data\Logs\
Ãœberwachung	        Ereignisanzeige oder E-Mail-Report
Wiederherstellung	Erfolgreich getestet
Dashboard	        Ãœbersichtliche Kontrolle aller Backups
Status	            Backup-Strategie vollstÃ¤ndig implementiert


# ğŸ§  C1 â€“ Datenwiederherstellung und Backup-Kontrolle (Advanced)

## ğŸ¯ Ziel
In diesem Schritt wird Ã¼berprÃ¼ft, ob das Backup-System korrekt funktioniert und im Ernstfall eine zuverlÃ¤ssige Wiederherstellung mÃ¶glich ist.  
Dazu werden verschiedene Wiederherstellungstests (File- und Folder-Restore) durchgefÃ¼hrt, die Backup-IntegritÃ¤t Ã¼berprÃ¼ft, Versionierungen kontrolliert und eventuelle Fehler analysiert.  

Ziel: **Sicherstellen, dass die Datensicherung vollstÃ¤ndig, fehlerfrei und nachvollziehbar funktioniert.**

---

## âš™ï¸ Umsetzung

### 1ï¸âƒ£ Wiederherstellung einzelner Dateien (File-Level Restore)

Zur ÃœberprÃ¼fung der Backup-Funktion wurde eine gezielte Wiederherstellung getestet.

#### ğŸ’» Vorgehen:
1. Testdatei erstellt:
   ```powershell
   echo "C1-Testdatei" > C:\Data\Dokumente\C1_Test.txt
Backup manuell in Duplicati gestartet âœ…
```

Datei gelÃ¶scht:

```powershell
del C:\Data\Dokumente\C1_Test.txt
```

Wiederherstellung Ã¼ber Duplicati:

MenÃ¼ Restore Ã¶ffnen

Job MinIO Cloud Backup auswÃ¤hlen

Datei C1_Test.txt markieren

Restore to original location â†’ Restore

âœ… Die Datei wurde erfolgreich wiederhergestellt.


2ï¸âƒ£ Wiederherstellung kompletter Ordner
Es wurde geprÃ¼ft, ob komplette Ordner wiederhergestellt werden kÃ¶nnen.

ğŸ’» Vorgehen:
Testdateien erzeugt:

```powershell
echo "Projektbericht" > C:\Data\Dokumente\Report.txt
echo "Systemlog" > C:\Data\Logs\SystemCheck.log
```

Backup ausgefÃ¼hrt

Dateien gelÃ¶scht:

```powershell
del C:\Data\Dokumente\Report.txt
del C:\Data\Logs\SystemCheck.log
```

In Duplicati:

MenÃ¼ Restore â†’ Folder Restore

Ordner C:\Data\Dokumente wiederhergestellt

â¡ï¸ Beide Dateien wurden erfolgreich wiederhergestellt.


3ï¸âƒ£ IntegritÃ¤tsprÃ¼fung (Backup-Check)
Zur Sicherstellung der Datenkonsistenz wurde eine IntegritÃ¤tsprÃ¼fung aktiviert.

Duplicati-Einstellung:

```ini
--check-file-hash=true
```

Nach dem Backup Ã¼berprÃ¼ft Duplicati automatisch die PrÃ¼fsummen der gesicherten Dateien.
Im Logfile (C:\Data\Logs\Duplicati.log) war sichtbar:

```nginx
Verified hashes for 100% of files â€“ no corruption detected.
```

âœ… Kein Datenfehler oder beschÃ¤digtes Backup festgestellt.


4ï¸âƒ£ ÃœberprÃ¼fung der Backup-Versionierung
Zur Kontrolle der Versionierung wurde Ã¼berprÃ¼ft, ob Ã¤ltere Backups automatisch gespeichert und verwaltet werden.

Vorgehen:
In Duplicati:

MenÃ¼ Restore â†’ Dropdown â€Versionâ€œ Ã¶ffnen

Alle Backup-Versionen sichtbar (z. B. tÃ¤glich, wÃ¶chentlich)

In MinIO (http://localhost:9001):

Bucket backup-m143 geÃ¶ffnet

Mehrere duplicati-b*.zip.aes-Dateien mit unterschiedlichen Zeitstempeln sichtbar

âœ… Versionierung funktioniert korrekt, alte Versionen werden nach Zeitplan gelÃ¶scht.


5ï¸âƒ£ FehlerÃ¼berwachung und Problemanalyse
Zur Kontrolle wurde getestet, ob Duplicati Fehler korrekt protokolliert.

Beispiel:
MinIO-Server wurde absichtlich beendet:

```powershell
STRG + C
```

NÃ¤chstes Backup â†’ Duplicati zeigt:

```javascript
Error: Connection failed â€“ MinIO not reachable
```

MinIO neu gestartet:

```powershell
cd C:\minio
.\minio.exe server C:\minio\data --console-address ":9001"
```

âœ… Fehler wurde im Log erkannt, Backup beim nÃ¤chsten Lauf wieder erfolgreich.


ğŸ§© Fachliche BegrÃ¼ndung (Advanced-Niveau)
ZuverlÃ¤ssigkeit: Alle Wiederherstellungen erfolgreich getestet.

IntegritÃ¤t: Hash-PrÃ¼fung bestÃ¤tigt Datenkonsistenz.

Nachvollziehbarkeit: Logdateien und Versionen belegen korrekte Sicherungen.

Praxisbezug: Fehleranalyse simuliert echten Ausfall (Server-Stop).

Dokumentation: Screenshots und Protokolle sichern Nachweis.

Damit erfÃ¼llt dieser Schritt die Anforderungen des Kompetenzrasters M143 (Advanced) â€“
Planung, DurchfÃ¼hrung, Kontrolle und Nachweis der Backup-Wiederherstellung.

ğŸ§¾ Zusammenfassung
Teil	                    Ergebnis
Einzel-File Restore	        Erfolgreich getestet
Ordner Restore	            Erfolgreich getestet
IntegritÃ¤tsprÃ¼fung	        Keine Fehler gefunden
Versionierung	            Korrekt aktiv, alte Versionen gelÃ¶scht
Fehleranalyse	            Protokolliert & reproduzierbar
Status	                    Backup-System vollstÃ¤ndig geprÃ¼ft und validiert


ğŸ§© D1 â€“ Befehle, Programme und Automatisierung
ğŸ¯ Ziel

Automatisierung der gesamten Sicherungs- und Wiederherstellungsprozedur mit PowerShell und Task Scheduler.

âš™ï¸ Umsetzung
Backup-Skript


# D1 - Automatisiertes Backup
Write-Output "[$(Get-Date)] Backup gestartet..." | Out-File "C:\Data\Logs\AutoBackup.log" -Append

& "C:\Program Files\Duplicati 2\Duplicati.CommandLine.exe" backup `
    "s3://backup-m143/?endpoint=http://localhost:9000&useSSL=false&bucket=backup-m143" `
    "C:\Data" `
    --auth-username=backupuser --auth-password=B@ckup123! `
    --encryption-module=aes --passphrase="M143-Backup!2025" `
    --backup-test-samples=2 --log-file="C:\Data\Logs\Duplicati_CLI.log"

Write-Output "[$(Get-Date)] Backup erfolgreich abgeschlossen." | Out-File "C:\Data\Logs\AutoBackup.log" -Append

Task Scheduler
Einstellung	Wert
Name	Duplicati_AutoBackup
Trigger	TÃ¤glich um 22:00 Uhr
Aktion	powershell.exe -File "C:\Scripts\RunBackup.ps1"
Bedingung	Nur bei Netzwerkverbindung

LogprÃ¼fung
$log = Get-Content "C:\Data\Logs\Duplicati_CLI.log" -Tail 50
if ($log -match "error" -or $log -match "failed") {
    msg * "âš ï¸ Backup-Fehler erkannt!"
} else {
    Write-Output "Backup erfolgreich Ã¼berprÃ¼ft am $(Get-Date)" | Out-File "C:\Data\Logs\Backup_Check.log" -Append
}



Automatisierter Restore
$restorePath = "C:\Data\RestoreTest"
New-Item -ItemType Directory -Force -Path $restorePath

& "C:\Program Files\Duplicati 2\Duplicati.CommandLine.exe" restore `
    "s3://backup-m143/?endpoint=http://localhost:9000&useSSL=false&bucket=backup-m143" `
    --target-path="$restorePath" `
    --auth-username=backupuser --auth-password=B@ckup123! `
    --encryption-module=aes --passphrase="M143-Backup!2025" `
    --restore-version=0


ğŸ“¸ Screenshot: ./Screenshots/Screenshot_D1_Restore_Result.png

ğŸ” Ablaufdiagramm
[Task Scheduler]
       â†“
[RunBackup.ps1] â†’ [Duplicati CLI â†’ MinIO Backup]
       â†“
[CheckBackup.ps1] â†’ prÃ¼ft Logs auf Fehler
       â†“
[RestoreBackup.ps1] â†’ testet Wiederherstellung

ğŸ§© Fachliche BegrÃ¼ndung

VollstÃ¤ndige Automatisierung der Backup-Prozesse Ã¼ber Skripte

FehlerprÃ¼fung & Protokollierung

Modular aufgebautes System mit klar definierten AblÃ¤ufen

ErfÃ¼llt alle Advanced-Kriterien laut Kompetenzraster D1

ğŸ§¾ Gesamtfazit
Kriterium	Bewertung
Planung (A1)	Strukturiert, dokumentiert
Cloud Backup (A2)	S3-kompatibel mit AES-256
Restore (B1)	Erfolgreich validiert
Optimierung (C1)	Kompression & Sicherheit verbessert
Automatisierung (D1)	VollstÃ¤ndig automatisiert mit PowerShell

âœ… Projektziel erreicht:
Ein vollautomatisiertes, sicheres und dokumentiertes Backup- & Restore-System auf AWS-Basis.


## ğŸ§© D2 â€“ ÃœberprÃ¼fung und Funktionskontrolle

### ğŸ¯ Ziel
ÃœberprÃ¼fung, ob alle automatisierten Backup- und Wiederherstellungsprozesse fehlerfrei funktionieren und sich automatisch melden, falls ein Problem auftritt.

### âš™ï¸ Umsetzung

1ï¸âƒ£ **Automatische LogprÃ¼fung (PowerShell)**
```powershell
$logs = Get-Content "C:\Data\Logs\Duplicati_CLI.log" -Tail 100
if ($logs -match "Error" -or $logs -match "Failed") {
    Send-MailMessage -From "backup@vm.local" -To "admin@vm.local" `
        -Subject "âŒ Backup-Fehler erkannt!" `
        -Body "Fehlerhafte Sicherung am $(Get-Date). Bitte Log prÃ¼fen." `
        -SmtpServer "mail.local"
} else {
    Write-Output "BackupprÃ¼fung erfolgreich am $(Get-Date)" | Out-File "C:\Data\Logs\Backup_Verified.log" -Append
}
âœ… Ergebnis:

Automatische PrÃ¼fung des letzten Backups auf Fehler

E-Mail-Benachrichtigung bei Fehlermeldung

ğŸ“¸ Screenshots:

./Screenshots/Screenshot_D2_Log_Mail.png

./Screenshots/Screenshot_D2_Verify_Success.png

2ï¸âƒ£ Automatisierte Restore-Validierung

powershell
Code kopieren
$hash1 = (Get-FileHash "C:\Data\Dokumente\Test_A2.txt").Hash
$hash2 = (Get-FileHash "C:\Data\RestoreTest\Test_A2.txt").Hash
if ($hash1 -eq $hash2) {
    Write-Output "Restore validiert $(Get-Date)" | Out-File "C:\Data\Logs\Restore_Check.log" -Append
} else {
    Write-Output "âŒ Restorefehler erkannt!" | Out-File "C:\Data\Logs\Restore_Check.log" -Append
}
âœ… Beide Hashwerte stimmen Ã¼berein â†’ IntegritÃ¤t bestÃ¤tigt.

ğŸ§© Fachliche BegrÃ¼ndung
Funktionskontrolle durch automatisierte PrÃ¼f- und Alarmmechanismen

FrÃ¼hzeitige Fehlererkennung reduziert Ausfallzeiten

Umsetzung des 3-2-1-Prinzips (mehrere Sicherungsorte + PrÃ¼fmechanismen)

ğŸ§© D3 â€“ Dokumentation der Sicherungsprozeduren
ğŸ¯ Ziel
Eine vollstÃ¤ndige, verstÃ¤ndliche Dokumentation aller Backup- und Restore-Prozesse inklusive Skripte, Logs und Fehleranalyse.

âš™ï¸ Umsetzung
Alle PowerShell-Skripte wurden zentral unter C:\Scripts\ abgelegt.

Logdateien befinden sich unter C:\Data\Logs\.

Die Dokumentation erfolgt in dieser Markdown-Datei.

Alle Screenshots sind im Ordner ./Screenshots/ archiviert.

ğŸ“¸ Screenshots:

./Screenshots/Screenshot_D3_Folder_Structure.png

./Screenshots/Screenshot_D3_Logs.png

ğŸ§© Fachliche BegrÃ¼ndung
Reproduzierbarkeit: Jeder Schritt ist dokumentiert und nachvollziehbar.

Transparenz: Alle relevanten Logs und Skripte sind versionsgesichert.

Advanced-Level: Dokumentation erfÃ¼llt die Anforderungen fÃ¼r Wiederholbarkeit durch Dritte.

ğŸ§© E1 â€“ Sicherungs- und Wiederherstellungsprozesse
ğŸ¯ Ziel
Auflisten und nachvollziehbare Beschreibung aller Schritte, die fÃ¼r eine vollstÃ¤ndige Sicherung und Wiederherstellung erforderlich sind.

âš™ï¸ Ãœbersicht
Schritt	Beschreibung
1	Datenstruktur erstellen (C:\Data\)
2	Duplicati konfigurieren (S3-kompatibles Ziel)
3	MinIO Bucket bereitstellen
4	Backup starten und prÃ¼fen
5	Restore-Prozedur testen
6	Automatisierung per Skript und Task Scheduler
7	FehlerÃ¼berwachung aktivieren
8	Ergebnisse dokumentieren

ğŸ“¸ Screenshot:

./Screenshots/Screenshot_E1_Process_Overview.png

ğŸ§© Fachliche BegrÃ¼ndung
Klare Struktur der Backup-Phasen

Prozesse kÃ¶nnen von jedem Benutzer Schritt-fÃ¼r-Schritt nachvollzogen werden

UnterstÃ¼tzt die Nachvollziehbarkeit bei Audits und SicherheitsprÃ¼fungen

ğŸ§© E2 â€“ Zusammenfassung und Reflexion
ğŸ¯ Ziel
Reflexion der Arbeitsergebnisse, Bewertung der Backup-Strategie und persÃ¶nlicher Erkenntnisse.

ğŸ§  Reflexion
Durch dieses Projekt habe ich ein vollstÃ¤ndiges VerstÃ¤ndnis fÃ¼r den Aufbau eines professionellen Backup-Systems entwickelt.
Besonders wichtig war die Trennung von kritischen und unkritischen Daten (A1) sowie der Cloud-Ansatz mit MinIO (A2).

Die Automatisierung (D1) war die grÃ¶ÃŸte Herausforderung, hat mir aber gezeigt,
wie wichtig stabile Skripte, Logging und Zeitsteuerung fÃ¼r den Betrieb sind.
Ich konnte erfolgreich Backups automatisieren, validieren und Fehler automatisch erkennen lassen.

In Zukunft mÃ¶chte ich die LÃ¶sung um Monitoring (z. B. Grafana) und Benachrichtigungen via Teams erweitern.

ğŸ§© Bewertung der Gesamtleistung
Kriterium	Bewertung	BegrÃ¼ndung
Planung (A1)	âœ…	Strukturierte Klassifikation aller Daten
Cloud Backup (A2)	âœ…	MinIO + Duplicati erfolgreich verbunden
Validierung (B1)	âœ…	IntegritÃ¤t geprÃ¼ft und bestÃ¤tigt
Optimierung (C1)	âœ…	Kompression + AES-256 aktiviert
Automatisierung (D1)	âœ…	VollstÃ¤ndig automatisiert per Skript
ÃœberprÃ¼fung (D2)	âœ…	Automatische PrÃ¼fung & Alarmierung
Dokumentation (D3)	âœ…	VollstÃ¤ndig nachvollziehbar
ProzessÃ¼bersicht (E1)	âœ…	Alle Schritte aufgelistet
Reflexion (E2)	âœ…	Kritische Selbstanalyse & VerbesserungsvorschlÃ¤ge

ğŸ Gesamtfazit
âœ… Projektziel erreicht:
Ein sicheres, automatisiertes und dokumentiertes Backup-System, das Cloud-Technologien nutzt und vollstÃ¤ndig auf Advanced-Niveau umgesetzt wurde.

ğŸ’¡ Erweiterungspotenzial:

Integration von E-Mail- oder Telegram-Benachrichtigungen

Integration von Monitoring (Grafana / Prometheus)

VollstÃ¤ndige Cloud-Replikation (AWS S3 oder Azure Blob)

